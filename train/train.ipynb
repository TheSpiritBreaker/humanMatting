{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution() # Speed\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, \\\n",
    "                                    Conv2DTranspose, BatchNormalization, \\\n",
    "                                    UpSampling2D, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGen(Sequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_batch = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        y_batch = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # 1:1, 4:3, 16:9, 2:1\n",
    "        # shapes = [(512, 384), (384, 512), (512, 288), (288, 512), (512, 256), (256, 512), (256, 256), (256, 256)]\n",
    "        # shape = shapes[np.random.choice(len(shapes))]\n",
    "        shape = (256, 256)\n",
    "        \n",
    "        x_ret, y_ret = [], []\n",
    "        for x_file_name, y_file_name in zip(x_batch, y_batch):\n",
    "            x_img = cv2.resize(cv2.imread(x_file_name, 0), shape)\n",
    "            y_img = cv2.resize(cv2.imread(y_file_name, cv2.IMREAD_UNCHANGED), shape)\n",
    "            probs = y_img[:,:,3] / 255.0\n",
    "            alpha_map = np.zeros((*probs.shape, 2))\n",
    "            alpha_map[:,:,0] = 1 - probs\n",
    "            alpha_map[:,:,1] = probs\n",
    "            x_ret.append(x_img[:,:,np.newaxis])\n",
    "            y_ret.append(alpha_map)\n",
    "\n",
    "        return np.array(x_ret), np.array(y_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikeUnet():\n",
    "    # input\n",
    "    im = Input(shape=(None, None, 1))\n",
    "    # conv1\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu')(im)\n",
    "    conv1 = MaxPooling2D(2)(x)\n",
    "    # conv2\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(conv1)\n",
    "    conv2 = MaxPooling2D(2)(x)\n",
    "    # conv3\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(conv2)\n",
    "    conv3 = MaxPooling2D(2)(x)\n",
    "    # conv4\n",
    "    x = Conv2D(128, 3, padding='same', activation='relu')(conv3)\n",
    "    conv4 = MaxPooling2D(2)(x)\n",
    "    # conv5\n",
    "    x = Conv2D(256, 3, padding='same', activation='relu')(conv4)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    # BatchNormalization\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    # trans-conv1\n",
    "    x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    # trans-conv2\n",
    "    x = Concatenate()([conv4, x])\n",
    "    x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    # trans-conv3\n",
    "    x = Concatenate()([conv3, x])\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    # trans-conv4\n",
    "    x = Concatenate()([conv2, x])\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(16, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    # trans-conv5\n",
    "    x = Concatenate()([conv1, x])\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(8, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    # output\n",
    "    x = Concatenate([im, x])\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu')(x)\n",
    "    out = Conv2D(2, 1, padding='same', activation='softmax')(x)\n",
    "    return Model(inputs=im, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_train.txt\", \"r\") as f:\n",
    "    x_train = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"y_train.txt\", \"r\") as f:\n",
    "    y_train = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"x_val.txt\", \"r\") as f:\n",
    "    x_val = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"y_val.txt\", \"r\") as f:\n",
    "    y_val = [filename[:-1] for filename in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = DataGen(x_train, y_train, batch_size=60)\n",
    "valGen = DataGen(x_val, y_val, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU\n",
    "class mIoU(MeanIoU):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__(num_classes=num_classes, name=\"mIoU\")\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().__call__(y_true, y_pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shadow\\anaconda3\\envs\\vision\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = LikeUnet()\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[mIoU(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "logdir = \"logs\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir, update_freq='batch', profile_batch=0)\n",
    "filepath = \"models\\\\night-{epoch:02d}-{val_mIoU:.4f}.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(filepath, monitor='val_mIoU', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "500/500 [==============================] - 445s 890ms/step - loss: 0.2819 - mIoU: 0.8024 - val_loss: 0.1424 - val_mIoU: 0.8949\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 426s 852ms/step - loss: 0.1065 - mIoU: 0.9237 - val_loss: 0.1542 - val_mIoU: 0.8871\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 423s 846ms/step - loss: 0.0793 - mIoU: 0.9441 - val_loss: 0.0841 - val_mIoU: 0.9398\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 425s 850ms/step - loss: 0.0669 - mIoU: 0.9535 - val_loss: 0.0714 - val_mIoU: 0.9507\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 412s 824ms/step - loss: 0.0591 - mIoU: 0.9593 - val_loss: 0.0725 - val_mIoU: 0.9500\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 413s 826ms/step - loss: 0.0537 - mIoU: 0.9636 - val_loss: 0.0649 - val_mIoU: 0.9540\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - 401s 802ms/step - loss: 0.0506 - mIoU: 0.9662 - val_loss: 0.0621 - val_mIoU: 0.9589\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - 405s 811ms/step - loss: 0.0480 - mIoU: 0.9679 - val_loss: 0.0579 - val_mIoU: 0.9602\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - 420s 840ms/step - loss: 0.0466 - mIoU: 0.9691 - val_loss: 0.0522 - val_mIoU: 0.9647\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - 407s 814ms/step - loss: 0.0440 - mIoU: 0.9710 - val_loss: 0.0483 - val_mIoU: 0.9685\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - 404s 809ms/step - loss: 0.0423 - mIoU: 0.9725 - val_loss: 0.0487 - val_mIoU: 0.9674\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - 423s 845ms/step - loss: 0.0407 - mIoU: 0.9736 - val_loss: 0.0504 - val_mIoU: 0.9669\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - 415s 831ms/step - loss: 0.0399 - mIoU: 0.9742 - val_loss: 0.0460 - val_mIoU: 0.9697\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - 411s 822ms/step - loss: 0.0383 - mIoU: 0.9755 - val_loss: 0.0502 - val_mIoU: 0.9681\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 420s 839ms/step - loss: 0.0383 - mIoU: 0.9757 - val_loss: 0.0438 - val_mIoU: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e5710e17b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=trainGen, validation_data=valGen, epochs=15, workers=8, shuffle=True,\n",
    "                    callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset for debugging\n",
    "# model.fit_generator(generator=valGen, epochs=2, workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('night.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('night.json', 'r') as f:\n",
    "    json_string = f.read()\n",
    "model = model_from_json(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}