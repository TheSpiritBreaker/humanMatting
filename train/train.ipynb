{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution() # Speed\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, \\\n",
    "                                    Conv2DTranspose, BatchNormalization, \\\n",
    "                                    UpSampling2D, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGen(Sequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_batch = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        y_batch = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        x_ret, y_ret = [], []\n",
    "        for x_file_name, y_file_name in zip(x_batch, y_batch):\n",
    "            x_img = cv2.resize(cv2.imread(x_file_name, 0), (256, 256))\n",
    "            y_img = cv2.resize(cv2.imread(y_file_name, cv2.IMREAD_UNCHANGED), (256, 256))\n",
    "            probs = y_img[:,:,3] / 255.0\n",
    "            alpha_map = np.zeros((*probs.shape, 2))\n",
    "            alpha_map[:,:,0] = 1 - probs\n",
    "            alpha_map[:,:,1] = probs\n",
    "            x_ret.append(x_img[:,:,np.newaxis])\n",
    "            y_ret.append(alpha_map)\n",
    "\n",
    "        return np.array(x_ret), np.array(y_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikeUnet():\n",
    "    # input\n",
    "    im = Input(shape=(None, None, 1))\n",
    "    # conv1\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu')(im)\n",
    "    conv1 = MaxPooling2D(2)(x)\n",
    "    # conv2\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(conv1)\n",
    "    conv2 = MaxPooling2D(2)(x)\n",
    "    # conv3\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(conv2)\n",
    "    conv3 = MaxPooling2D(2)(x)\n",
    "    # conv4\n",
    "    x = Conv2D(128, 3, padding='same', activation='relu')(conv3)\n",
    "    conv4 = MaxPooling2D(2)(x)\n",
    "    # conv5\n",
    "    x = Conv2D(256, 3, padding='same', activation='relu')(conv4)\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    # BatchNormalization\n",
    "    x = BatchNormalization()(x)\n",
    "    # trans-conv1\n",
    "    x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(128, 3, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    # trans-conv2\n",
    "    x = Concatenate()([conv4, x])\n",
    "    x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    # trans-conv3\n",
    "    x = Concatenate()([conv3, x])\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    # trans-conv4\n",
    "    x = Concatenate()([conv2, x])\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(16, 3, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    # trans-conv5\n",
    "    x = Concatenate()([conv1, x])\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(8, 3, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    # output\n",
    "    x = Conv2D(4, 3, padding='same', activation='relu')(x)\n",
    "    out = Conv2D(2, 1, padding='same', activation='softmax')(x)\n",
    "    return Model(inputs=im, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_train.txt\", \"r\") as f:\n",
    "    x_train = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"y_train.txt\", \"r\") as f:\n",
    "    y_train = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"x_val.txt\", \"r\") as f:\n",
    "    x_val = [filename[:-1] for filename in f.readlines()]\n",
    "with open(\"y_val.txt\", \"r\") as f:\n",
    "    y_val = [filename[:-1] for filename in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = DataGen(x_train, y_train, batch_size=60)\n",
    "valGen = DataGen(x_val, y_val, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU\n",
    "class mIoU(MeanIoU):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__(num_classes=num_classes, name=\"mIoU\")\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().__call__(y_true, y_pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LikeUnet()\n",
    "# with open('dark.json', 'w') as f:\n",
    "#     f.write(model.to_json())\n",
    "with open('dark.json', 'r') as f:\n",
    "    json_string = f.read()\n",
    "model = model_from_json(json_string)\n",
    "model.load_weights('models\\\\dark-20-0.9774.h5')\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[mIoU(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "logdir = \"logs\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir, update_freq='batch', profile_batch=0)\n",
    "filepath = \"models\\\\dark-{epoch:02d}-{val_mIoU:.4f}.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(filepath, monitor='val_mIoU', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset for debugging\n",
    "# model.fit_generator(generator=valGen, epochs=2, workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=trainGen, validation_data=valGen, epochs=30, workers=8, shuffle=True,\n",
    "                    callbacks=[tensorboard_callback, checkpoint_callback], initial_epoch=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
